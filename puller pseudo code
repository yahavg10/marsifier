import boto3
import redis
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, as_completed
from botocore.config import Config
from botocore.exceptions import ClientError


class S3Processor:
    def __init__(self, region, source_bucket, target_bucket, redis_host, redis_port):
        self.region = region
        self.source_bucket = source_bucket
        self.target_bucket = target_bucket

        # Initialize AWS and Redis clients
        self.s3_client = self._init_s3_client()
        self.redis_client = self._init_redis_client(redis_host, redis_port)

        # Configuration for processing
        self.max_file_threads = 20  # Threads per page for file processing
        self.redis_key_prefix = "processed:"

    def _init_s3_client(self):
        """Initialize the S3 client with transfer acceleration."""
        return boto3.client(
            "s3",
            region_name=self.region,
            config=Config(s3={"use_accelerate_endpoint": True}),
        )

    def _init_redis_client(self, host, port):
        """Initialize the Redis client."""
        return redis.StrictRedis(host=host, port=port, decode_responses=True)

    def file_already_processed(self, s3_key):
        """Check Redis to see if a file has already been processed."""
        return self.redis_client.exists(f"{self.redis_key_prefix}{s3_key}")

    def mark_file_processed(self, s3_key):
        """Mark a file as processed in Redis."""
        self.redis_client.set(f"{self.redis_key_prefix}{s3_key}", "1")

    def fetch_file_from_s3(self, s3_key):
        """Fetch file from S3 and return its content as a stream."""
        try:
            response = self.s3_client.get_object(Bucket=self.source_bucket, Key=s3_key)
            return response["Body"]
        except ClientError as e:
            print(f"Error fetching file {s3_key} from S3: {e}")
            return None

    def decrypt_content(self, file_stream):
        """Simulate decryption of the file content."""
        decrypted_content = b""
        for chunk in file_stream.iter_chunks(chunk_size=1024 * 1024):  # 1 MB chunks
            decrypted_content += chunk  # Replace with actual decryption logic
        return decrypted_content

    def upload_to_target_s3(self, s3_key, content):
        """Upload decrypted content to the target S3 bucket."""
        try:
            self.s3_client.put_object(
                Bucket=self.target_bucket, Key=s3_key, Body=content
            )
            print(f"File uploaded successfully: {s3_key}")
        except ClientError as e:
            print(f"Error uploading file {s3_key} to target S3 bucket: {e}")

    def process_file(self, s3_key):
        """Fetch, decrypt, and upload a file."""
        if self.file_already_processed(s3_key):
            print(f"File already processed: {s3_key}")
            return

        print(f"Processing file: {s3_key}")
        file_stream = self.fetch_file_from_s3(s3_key)
        if not file_stream:
            return

        decrypted_content = self.decrypt_content(file_stream)
        self.upload_to_target_s3(s3_key, decrypted_content)
        self.mark_file_processed(s3_key)

    def process_page(self, page):
        """Process all files in a page using threads."""
        if "Contents" not in page:
            return

        with ThreadPoolExecutor(max_workers=self.max_file_threads) as executor:
            future_to_key = {}
            for obj in page["Contents"]:
                s3_key = obj["Key"]
                future_to_key[executor.submit(self.process_file, s3_key)] = s3_key

            # Wait for all threads in this page to complete
            for future in as_completed(future_to_key):
                try:
                    future.result()
                except Exception as exc:
                    print(f"Error in thread for file {future_to_key[future]}: {exc}")

    def list_pages(self, prefix):
        """List pages of files from the S3 bucket."""
        operation_parameters = {"Bucket": self.source_bucket, "Prefix": prefix}

        try:
            paginator = self.s3_client.get_paginator("list_objects_v2")
            page_iterator = paginator.paginate(**operation_parameters)

            for page in page_iterator:
                self.process_page(page)

        except ClientError as e:
            print(f"S3 Pagination error: {e}")


def list_and_process_pages(region, source_bucket, target_bucket, redis_host, redis_port, prefix):
    """Create an S3Processor and process pages in a separate process."""
    processor = S3Processor(region, source_bucket, target_bucket, redis_host, redis_port)
    processor.list_pages(prefix)


def main():
    region = "us-west-2"
    source_bucket = "my-source-bucket"
    target_bucket = "my-target-bucket"
    redis_host = "memorydb-cluster.xxxxx.clustercfg.memorydb.us-west-2.amazonaws.com"
    redis_port = 6379
    prefix = "foo/baz/"

    # Start multiprocessing to process each page
    while True:
        with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:
            pool.apply_async(list_and_process_pages, (region, source_bucket, target_bucket, redis_host, redis_port, prefix))
            pool.close()
            pool.join()


if __name__ == "__main__":
    main()